{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3c8f7-61ae-40a7-9129-f2f2a51441ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the Vertex AI Python SDK\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, \n",
    "              location=REGION, \n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcdc46-04d1-48c3-9662-0a18ec0f34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"Missing flamingo discovered at swimming pool\"\n",
    "\n",
    "in_2 = \"Sea otter spotted on surfboard by beach\"\n",
    "\n",
    "in_3 = \"Baby panda enjoys boat ride\"\n",
    "\n",
    "\n",
    "in_4 = \"Breakfast themed food truck beloved by all!\"\n",
    "\n",
    "in_5 = \"New curry restaurant aims to please!\"\n",
    "\n",
    "\n",
    "in_6 = \"Python developers are wonderful people\"\n",
    "\n",
    "in_7 = \"TypeScript, C++ or Java? All are great!\" \n",
    "\n",
    "\n",
    "input_text_lst_news = [in_1, in_2, in_3, in_4, in_5, in_6, in_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33966415-a6fe-46dd-8695-0f8b9799966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f23a6-443a-49b7-abe3-a82dcc6a4ae3",
   "metadata": {},
   "source": [
    "- Get embeddings for all pieces of text.\n",
    "- Store them in a 2D NumPy array (one row for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8345f-689b-4c31-8697-c0f63d2002f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_news:\n",
    "    emb = embedding_model.get_embeddings(\n",
    "        [input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "    \n",
    "embeddings_array = np.array(embeddings) \n",
    "print(\"Shape: \" + str(embeddings_array.shape))\n",
    "print(embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42156e-1037-443d-94bd-942644dde3f3",
   "metadata": {},
   "source": [
    "#### Reduce embeddings from 768 to 2 dimensions for visualization\n",
    "- We'll use principal component analysis (PCA).\n",
    "- You can learn more about PCA in [this video](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/73zWO/reducing-the-number-of-features-optional) from the Machine Learning Specialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af3fe88-8a44-43ad-8f5f-f7e9ecea2ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Perform PCA for 2D visualization\u001b[39;00m\n\u001b[1;32m      4\u001b[0m PCA_model \u001b[38;5;241m=\u001b[39m PCA(n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for 2D visualization\n",
    "PCA_model = PCA(n_components = 2)\n",
    "PCA_model.fit(embeddings_array)\n",
    "new_values = PCA_model.transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a55297-8ce1-4769-9cd4-8579a3977c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl\n",
    "\n",
    "from utils import plot_2D\n",
    "plot_2D(new_values[:,0], new_values[:,1], input_text_lst_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ddf1a-02f4-4459-aa7b-ed187c3efe89",
   "metadata": {},
   "source": [
    "#### Embeddings and Similarity\n",
    "- Plot a heat map to compare the embeddings of sentences that are similar and sentences that are dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec827c52-6296-40b0-87df-285c4b9cf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"\"\"He couldnâ€™t desert \n",
    "          his post at the power plant.\"\"\"\n",
    "\n",
    "in_2 = \"\"\"The power plant needed \n",
    "          him at the time.\"\"\"\n",
    "\n",
    "in_3 = \"\"\"Cacti are able to \n",
    "          withstand dry environments.\"\"\" \n",
    "\n",
    "in_4 = \"\"\"Desert plants can \n",
    "          survive droughts.\"\"\" \n",
    "\n",
    "input_text_lst_sim = [in_1, in_2, in_3, in_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85760619-63b8-425c-b31d-cf0b796a9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_sim:\n",
    "    emb = embedding_model.get_embeddings([input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "    \n",
    "embeddings_array = np.array(embeddings) \n",
    "\n",
    "from utils import plot_heatmap\n",
    "\n",
    "y_labels = input_text_lst_sim\n",
    "\n",
    "# Plot the heatmap\n",
    "plot_heatmap(embeddings_array, y_labels = y_labels, title = \"Embeddings Heatmap\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21625ed9-4bd7-4751-be56-76a665f4cc0e",
   "metadata": {},
   "source": [
    "Note: the heat map won't show everything because there are 768 columns to show.  To adjust the heat map with your mouse:\n",
    "- Hover your mouse over the heat map.  Buttons will appear on the left of the heatmap.  Click on the button that has a vertical and horizontal double arrow (they look like axes).\n",
    "- Left click and drag to move the heat map left and right.\n",
    "- Right click and drag up to zoom in.\n",
    "- Right click and drag down to zoom out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa57d5b-8ec2-4adc-a0ec-9905ddcf5f3b",
   "metadata": {},
   "source": [
    "#### Compute cosine similarity\n",
    "- The `cosine_similarity` function expects a 2D array, which is why we'll wrap each embedding list inside another list.\n",
    "- You can verify that sentence 1 and 2 have a higher similarity compared to sentence 1 and 4, even though sentence 1 and 4 both have the words \"desert\" and \"plant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b81d0d-e3b2-4841-9416-df3d5ee19ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare(embeddings,idx1,idx2):\n",
    "    return cosine_similarity([embeddings[idx1]],[embeddings[idx2]])\n",
    "\n",
    "print(in_1)\n",
    "print(in_2)\n",
    "print(compare(embeddings,0,1))\n",
    "\n",
    "print(in_1)\n",
    "print(in_4)\n",
    "print(compare(embeddings,0,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
